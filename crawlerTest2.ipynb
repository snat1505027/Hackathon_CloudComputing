{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.lyrics.com'\n",
    "r = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.headers['content-type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs4.BeautifulSoup(r.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = [a.attrs.get('href') for a in soup.select('a[href]')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "links = list(filter(None,links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urldefrag, urlparse, urljoin\n",
    "links = [urldefrag(link)[0] for link in links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = [urldefrag(link)[0] for link in links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = list(filter(None,links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links2 = [link if bool(urlparse(link).netloc) else urljoin(url,link) for link in links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "singledomain = True\n",
    "base_domain = 'www.lyrics.com'\n",
    "if singledomain:\n",
    "    links2 = [link for link in links2 if (urlparse(link).netloc == base_domain)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "print(links2[i])\n",
    "print(urlparse(links2[i]).netloc==base_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_in_list(url, listobj):\n",
    "    \"\"\"Determine whether a URL is in a list of URLs.\n",
    "    This function checks whether the URL is contained in the list with either\n",
    "    an http:// or https:// prefix. It is used to avoid crawling the same\n",
    "    page separately as http and https.\n",
    "    \"\"\"\n",
    "    http_version = url.replace('https://', 'http://')\n",
    "    https_version = url.replace('http://', 'https://')\n",
    "    return (http_version in listobj) or (https_version in listobj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getlinks(pageurl, domain, soup, base_domain):\n",
    "    \"\"\"Returns a list of links from from this page to be crawled.\n",
    "    pageurl = URL of this page\n",
    "    domain = domain being crawled (None to return links to *any* domain)\n",
    "    soup = BeautifulSoup object for this page\n",
    "    \"\"\"\n",
    "\n",
    "    # get target URLs for all links on the page\n",
    "    links = [a.attrs.get('href') for a in soup.select('a[href]')]\n",
    "\n",
    "    # remove fragment identifiers\n",
    "    links = [urldefrag(link)[0] for link in links]\n",
    "\n",
    "    # remove any empty strings\n",
    "    links = [link for link in links if link]\n",
    "\n",
    "    # if it's a relative link, change to absolute\n",
    "    links = [link if bool(urlparse(link).netloc) else urljoin(pageurl, link) \\\n",
    "        for link in links]\n",
    "    # if only crawing a single domain, remove links to other domains\n",
    "    if domain:\n",
    "        #print('domain-checked')\n",
    "        links = [link for link in links if (base_domain in link)]\n",
    "    #print(links)\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## https://github.com/dmahugh/crawlerino/blob/master/crawlerino.py\n",
    "import collections\n",
    "import string\n",
    "\n",
    "from timeit import default_timer\n",
    "from urllib.parse import urldefrag, urljoin, urlparse\n",
    "\n",
    "import bs4\n",
    "import requests\n",
    "\n",
    "def crawler(startpage, base_domain, maxpages=100, singledomain=True):\n",
    "    \"\"\"Crawl the web starting from specified page.\n",
    "    1st parameter = URL of starting page\n",
    "    maxpages = maximum number of pages to crawl\n",
    "    singledomain = whether to only crawl links within startpage's domain\n",
    "    \"\"\"\n",
    "    #base_domain = startpage\n",
    "    pagequeue = collections.deque() # queue of pages to be crawled\n",
    "    pagequeue.append(startpage)\n",
    "    crawled = [] # list of pages already crawled\n",
    "    domain = urlparse(startpage).netloc if singledomain else None\n",
    "    #print(domain)\n",
    "\n",
    "    pages = 0 # number of pages succesfully crawled so far\n",
    "    failed = 0 # number of links that couldn't be crawled\n",
    "\n",
    "    #sess = requests.session() # initialize the session\n",
    "    while pages < maxpages and pagequeue:\n",
    "        url = pagequeue.popleft() # get next page to crawl (FIFO queue)\n",
    "        print(url)\n",
    "        # read the page\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "        except (requests.exceptions.MissingSchema,\n",
    "                requests.exceptions.InvalidSchema):\n",
    "            print(\"*FAILED*:\", url)\n",
    "            failed += 1\n",
    "            continue\n",
    "        if not response.headers['content-type'].startswith('text/html'):\n",
    "            print(\"not html. skipping..\")\n",
    "            continue # don't crawl non-HTML content\n",
    "\n",
    "        # Note that we create the Beautiful Soup object here (once) and pass it\n",
    "        # to the other functions that need to use it\n",
    "        soup = bs4.BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # process the page\n",
    "        crawled.append(url)\n",
    "        pages += 1\n",
    "        #print(soup)\n",
    "        # get the links from this page and add them to the crawler queue\n",
    "        links = getlinks(url, domain, soup, base_domain)\n",
    "        #print(links)\n",
    "        for link in links:\n",
    "            if not url_in_list(link, crawled) and not url_in_list(link, pagequeue):\n",
    "                pagequeue.append(link)\n",
    "\n",
    "    print('{0} pages crawled, {1} links failed.'.format(pages, failed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crawler('https://www.lyrics.com/lyrics/in%20the%20end', 'https://www.lyrics.com/lyrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(url)\n",
    "# read the page\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "except (requests.exceptions.MissingSchema,\n",
    "        requests.exceptions.InvalidSchema):\n",
    "    print(\"*FAILED*:\", url)\n",
    "    failed += 1\n",
    "    continue\n",
    "if not response.headers['content-type'].startswith('text/html'):\n",
    "    print(\"not html. skipping..\")\n",
    "\n",
    "# Note that we create the Beautiful Soup object here (once) and pass it\n",
    "# to the other functions that need to use it\n",
    "soup = bs4.BeautifulSoup(response.text, \"html.parser\")\n",
    "# process the page\n",
    "#pages += 1\n",
    "#print(soup)\n",
    "# get the links from this page and add them to the crawler queue\n",
    "links = getlinks(url, domain, soup, base_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getlinks(pageurl, domain, soup, base_domain):\n",
    "    \"\"\"Returns a list of links from from this page to be crawled.\n",
    "    pageurl = URL of this page\n",
    "    domain = domain being crawled (None to return links to *any* domain)\n",
    "    soup = BeautifulSoup object for this page\n",
    "    \"\"\"\n",
    "\n",
    "    # get target URLs for all links on the page\n",
    "    links = [a.attrs.get('href') for a in soup.select('a[href]')]\n",
    "\n",
    "    # remove fragment identifiers\n",
    "    links = [urldefrag(link)[0] for link in links]\n",
    "\n",
    "    # remove any empty strings\n",
    "    links = [link for link in links if link]\n",
    "\n",
    "    # if it's a relative link, change to absolute\n",
    "    links = [link if bool(urlparse(link).netloc) else urljoin(pageurl, link) \\\n",
    "        for link in links]\n",
    "    # if only crawing a single domain, remove links to other domains\n",
    "    if domain:\n",
    "        #print('domain-checked')\n",
    "        links = [link for link in links if (base_domain in link)]\n",
    "    #print(links)\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.lyrics.com/lyrics/in%20the%20end'\n",
    "print(url)\n",
    "# read the page\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "except (requests.exceptions.MissingSchema,\n",
    "        requests.exceptions.InvalidSchema):\n",
    "    print(\"*FAILED*:\", url)\n",
    "    failed += 1\n",
    "if not response.headers['content-type'].startswith('text/html'):\n",
    "    print(\"not html. skipping..\")\n",
    "\n",
    "soup = bs4.BeautifulSoup(response.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#soup.find_all('a')\n",
    "#soup.find_all('div')[]\n",
    "#soup.find\n",
    "i=0\n",
    "soup.body.find_all('div', attrs={'class':'sec-lyric clearfix'})[i].find_all('div', attrs={'class':'lyric-meta within-lyrics'})[0].p#.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_title_and_artist(soup, base_title, base_url='https://www.lyrics.com', base_artist=None):\n",
    "    temp = soup.body.find_all('div', attrs={'class':'sec-lyric clearfix'})\n",
    "    #print(temp)\n",
    "    for t in temp:\n",
    "        base = t.find_all('div', attrs={'class':'lyric-meta within-lyrics'})[0]\n",
    "        title = base.find('p', attrs={'class':'lyric-meta-title'}).text\n",
    "        artist = base.find('p', attrs={'class':'lyric-meta-artists'}).text\n",
    "        link = base.find('p', attrs={'class':'lyric-meta-title'}).a.get('href')\n",
    "        link = base_url + link\n",
    "        #print(title, \" -- \" ,base_title)\n",
    "        if base_artist is not None:\n",
    "            if base_artist.lower() != artist.lower():\n",
    "                print(base_artist)\n",
    "                continue\n",
    "        if title.lower() == base_title.lower():\n",
    "            print(title,\" \", artist, \" \", link )\n",
    "            return [title, artist, link]\n",
    "            break     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#title, artist, link = find_title_and_artist(soup, base_title=\"in the end\", base_artist=\"bee gees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_lyrics_title_artist(keyword, base_artist=None):\n",
    "    url = 'https://www.lyrics.com/lyrics/'+keyword.replace(\" \", \"%20\")\n",
    "    #print(url)\n",
    "    # read the page\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "    except (requests.exceptions.MissingSchema,\n",
    "            requests.exceptions.InvalidSchema):\n",
    "        print(\"*FAILED*:\", url)\n",
    "    if not response.headers['content-type'].startswith('text/html'):\n",
    "        print(\"not html. skipping..\")\n",
    "        return\n",
    "    soup = bs4.BeautifulSoup(response.text, \"html.parser\")\n",
    "    title, artist, link = find_title_and_artist(soup, base_title=keyword, base_artist=base_artist)\n",
    "    lyrics = find_lyric(link)\n",
    "    return [title, artist, link, lyrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the End   Bee Gees   https://www.lyrics.com/lyric/18728619/Bee+Gees/In+the+End\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['In the End',\n",
       " 'Bee Gees',\n",
       " 'https://www.lyrics.com/lyric/18728619/Bee+Gees/In+the+End',\n",
       " \"I've come home \\nI've been away too long \\nBut you don't want me back again \\nAnd I tried, in the end \\nI can't say you made me very sad \\nThe point of love is not to lie \\nPoints denied, in the end, in the end \\nHow can you tell me you don't want me \\nWhen I've done all I can to save the day \\nSuppose I'm in the way \\nIn the end, in the end, in the end \\nYou don't (don't) see sense, eyes never cry \\nPlease take part of me, leave the rest to die \\nI can't say you made me very sad \\nThe point of love is not to lie \\nPoints denied, in the end, in the end \\nHow can you tell me you don't want me \\nWhen I've done all I can to save the day \\nSuppose I'm in the way \\nIn the end, in the end \\nHow can you tell me you don't want me \\nWhen I've done all I can to save the day \\nSuppose I'm in the way \\nIn the end, in the end, in the end\"]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_lyrics_title_artist(\"in the end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lyric(link):\n",
    "    response = requests.get(link)\n",
    "    if not response.headers['content-type'].startswith('text/html'):\n",
    "        print(\"not html. skipping..\")\n",
    "        #return\n",
    "    soup = bs4.BeautifulSoup(response.text, \"html.parser\")\n",
    "    lyrics = soup.body.pre.text.replace(\"\\r\\n\",' \\n')\n",
    "    #print(lyrics)\n",
    "    return lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_lyric(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
